{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done by Linan Qiu [github.com/linanqiu](github.com/linanqiu) for Apoorv Agarwal and Owen Rambow in partial satisfaction of the requirements of the COMS W3998 Undergraduate Research Course.\n",
    "\n",
    "As opposed to the research log, this is more about what I learned personally and the various detours I took during this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codeword Detection\n",
    "\n",
    "The codeword detection project was the mainstay of this semester.\n",
    "\n",
    "## Replicating Previous Work and Word Similarity Model\n",
    "\n",
    "1. (1 Week) I took over the codeword detection project, replicating the word done by Mariyam\n",
    "2. (1 Week) A naive comparison of cosine distances between words proved to be too noisy to be feasible. Started thinking of alternatives to the cosine distance comparison method. At the same time, talked to Karl Stratos regarding an alternative to Word2Vec.\n",
    "3. (1 Week) Turns out a simple comparison of word similarity (which theoretically is similar to cosine distance, but is more noise tolerant) was able to reveal codewords (with high false positives) on the Enron corpus.\n",
    "\n",
    "At this point, Apoorv and I realized that a synthetic dataset need to be created for efficient further testing, since actually testing codewords from the Enron dataset required a very large reference corpus (so that the words like `raptor` appeared frequently enough).\n",
    "\n",
    "## Synthetic Dataset Creation\n",
    "\n",
    "1. (1 Week) Successfully parsed the WSJ corpus using undocumented code in NLTK. Created a synthetic dataset using the TF-IDF method documented in the research log. Found this to produce rather nonsensical words.\n",
    "2. (1 Week) Improved on the TF-IDF method by using a Gamma distribution to sample words from the dataset (effectively awarding words that have high TF-IDF, but penalizing words that appear way too infrequently). From the WSJ dataset, we were able to obtain words such as `yen`, `credit`, `stake` etc.\n",
    "3. (1 Week) Wrote a routine to properly perform substitution, run Word2Vec on an original and substituted corpus, save the results accordingly and retrieve them systematically.\n",
    "4. (1 Week) Performed analysis on the results of the synthetic codeword dataset, showing high accuracy in codeword detection.\n",
    "5. (1 Week) Retrieved matching between codewords and original words to a high degree.\n",
    "6. (1 Week) Compiled results into the research log.\n",
    "\n",
    "# Literature\n",
    "\n",
    "I have not had time last semester to digest much of the literature that Apoorv has sent me. This is largely my fault -- I have been overly focused on the engineering aspects of this project and neglected the theoretical portions. This will be a focus of this semester.\n",
    "\n",
    "\n",
    "# GPU\n",
    "\n",
    "1. (1 Week) Along the way, I tried to use the GPU accelerated version of Word2Vec. Turns out the ones that were celebrated produced nonsensical results. This wasted a week of time.\n",
    "2. (Currently low priority) Reproduce Word2Vec using TensorFlow.\n",
    "\n",
    "# Challenges\n",
    "\n",
    "- Python: I was new to Python, and that resulted in a rather slow pace of progress in code. This has been largely resolved and I'm now more than comfortable in the language.\n",
    "- Literature: I will need to read more papers this semester if I want to get this progress submitted to ACL2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
